{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8911b05-25f1-4663-8b1f-5b41700edc5a",
   "metadata": {},
   "source": [
    "This notebook provides a summary of our approach to the project, with any issues we face and limitations/assumptions made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "527b8a72-c70a-41e6-9afc-71209585fb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "data_directory = \"../data/curated/\"\n",
    "merchants = pd.read_parquet(data_directory + \"merchants.parquet\")\n",
    "transactions = pd.read_parquet(data_directory + \"transactions.parquet\")\n",
    "consumers = pd.read_parquet(data_directory + \"consumers.parquet\")\n",
    "census = pd.read_csv(data_directory + \"census.csv\")\n",
    "#merged_transactions = pd.read_parquet(data_directory + \"merged_transactions.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c06c27-214f-4722-b463-e33fbb62de9f",
   "metadata": {},
   "source": [
    "# Normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d9979d-0ae5-48f2-a8ee-9b2c6e954877",
   "metadata": {},
   "source": [
    "Firstly, we noticed an issue with the merchant dataset: the 'tags' column was incorrectly constructed with an overload of information - it required normalisation, not only because it showcases technical competency, but the separation could be useful in ranking later on (for example, take-rate, revenue band and category of purchase may all be relevant when ranking and, ideally, should be in different columns).\n",
    "\n",
    "Hence, a normalisation procedure was conducted on this column and the corresponding ETL implementation was done, too. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "099ddeec-f13e-40d9-abbc-9eeb5b830952",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### INSERT NORMALISED MERCHANT TABLE HERE - NOT NECESSARY, BUT NOTEBOOK MAY LOOK BETTER ####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7842842-29da-4284-bca6-247a08dd6f4a",
   "metadata": {},
   "source": [
    "# Preprocessing/Outlier Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd59627-2cad-4669-b45e-df28a24ce276",
   "metadata": {},
   "source": [
    "After downloading/extracting the relevant datasets that were provided to us and conducting some preliminary analysis, the first step was to examine outliers and determine a method to eliminate them from the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bd6d18-6883-4ad0-b603-020e08da3950",
   "metadata": {},
   "source": [
    "We found that, after merging transactions with merchants, the nulls are predominantly related to the name of the merchant and what they purchased ('tags'). At the preliminary stage, it was determined that these can be considered vital features when ranking merchants, and if we lack this data, we hinder the accuracy of our selection metrics. Hence, all nulls were removed from further analysis.\n",
    "\n",
    "To conduct further outlier analysis, we utilised a box plot (per merchant) to visualise the anomalies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278cf7c8-179f-4062-be2c-592002bdc109",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_noNull = transactions.merge(merchants, on='merchant_abn').dropna().groupby(\"merchant_abn\").agg(outlier_count=(\"dollar_value\", get_outliers))\n",
    "\n",
    "def get_outliers(col):\n",
    "    q1 = np.percentile(column, 25)\n",
    "    q3 = np.percentile(column, 75) \n",
    "    IQR = q3 - q1\n",
    "    return sum((column<(q1 - IQR)) | (column>(q3 + IQR)))\n",
    "plt.figure(figsize=(10,10))\n",
    "#sns.boxplot(x='name', y='dollar_value', data = transactions_noNull)\n",
    "\n",
    "\n",
    "#### INSERT BOX-PLOT/VISUALISATION OF OUTLIERS HERE ####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e788bb3-f4b4-4610-ab3f-c8115fbbf521",
   "metadata": {},
   "source": [
    "To eliminate the observed outliers, an IQR statistical implementation was used, resulting in approximately 3% of all data (113982 rows) being removed.\n",
    "\n",
    "Complementing all these steps, a generic ETL script was being updated using separate functions for obtaining and preprocessing the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31bc3b5-7633-420d-80dc-12d440ebc8f5",
   "metadata": {},
   "source": [
    "### Using a Fraud Model to Further Remove Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b803cab-ed40-4c03-aad3-c59ec3f63254",
   "metadata": {},
   "source": [
    "After the release of the fraud dataset, we wanted to implement a model that detects fraudulent transactions and also removes them from the dataset.\n",
    "\n",
    "In order to identify outliers, we had to normalise the dollar values according to each customer using Median and Quantile Scaling, and then train a linear regression model on the transaction-fraud data. Then, we would apply this model to the outliers (dollar values >$2 after the scaling) in the rest of the dataset. In the end, we used the predicted probability in conjunction with a randomly generated probability to remove the fraud data (for example, if probability of fraud is 60%, then there is a 60% chance it will be removed from the dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6635bd1-a756-424a-8d53-78bf0d335354",
   "metadata": {},
   "source": [
    "### External Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b88f2a7-8f57-41c2-a1f5-44cec4f142f0",
   "metadata": {},
   "source": [
    "The external dataset (census data) was retrieved from the ABS website. We believed that some of these features could prove useful in deriving a ranking model. Additionally, as the census data linked customer data by postcode and gender, it is assumed that this average is representative of the individual.\n",
    "\n",
    "Due to the hundreds of features available, feature engineering was done in order to obtain the most predictive attributes of a consumer. For example, a house-repayment-to-income ratio was deemed important as it showcases how risky - or how likely to default - a consumer is, and was engineered by dividing median mortgage payment of a consumer by their median income.\n",
    "\n",
    "The data was then cleaned and merged into other datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c723e95-35ab-44b1-b5f1-07862b43dca4",
   "metadata": {},
   "source": [
    " ### Final (Merged) Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef52ebb9-4ef7-4fbb-a3d3-508ab6b410c5",
   "metadata": {},
   "source": [
    "By the preprocessing and outlier analysis detailed above for each dataset, we were able to obtain a final dataset by merging.\n",
    "\n",
    "The ETL script was finalised, also. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1d63c17f-9607-42b9-b55a-6eeb0d751392",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'int' and 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [26]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m merged_transactions \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_directory\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmerged_transactions.parquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parquet.py:493\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, **kwargs)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;124;03mLoad a parquet object from the file path, returning a DataFrame.\u001b[39;00m\n\u001b[1;32m    448\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[38;5;124;03mDataFrame\u001b[39;00m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    491\u001b[0m impl \u001b[38;5;241m=\u001b[39m get_engine(engine)\n\u001b[0;32m--> 493\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parquet.py:347\u001b[0m, in \u001b[0;36mFastParquetImpl.read\u001b[0;34m(self, path, columns, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m    343\u001b[0m     path \u001b[38;5;241m=\u001b[39m handles\u001b[38;5;241m.\u001b[39mhandle\n\u001b[1;32m    345\u001b[0m parquet_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mParquetFile(path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparquet_kwargs)\n\u001b[0;32m--> 347\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mparquet_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    350\u001b[0m     handles\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/fastparquet/api.py:752\u001b[0m, in \u001b[0;36mParquetFile.to_pandas\u001b[0;34m(self, columns, categories, filters, index, row_filter)\u001b[0m\n\u001b[1;32m    750\u001b[0m         start \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m rg\u001b[38;5;241m.\u001b[39mnum_rows\n\u001b[1;32m    751\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 752\u001b[0m     size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_rows\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrgs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    753\u001b[0m     selected \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(rgs)  \u001b[38;5;66;03m# just to fill zip, below\u001b[39;00m\n\u001b[1;32m    754\u001b[0m df, views \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_allocate(size, columns, categories, index)\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'int' and 'dict'"
     ]
    }
   ],
   "source": [
    "#merged_transactions = pd.read_parquet(data_directory + \"merged_transactions.parquet\")\n",
    "\n",
    "\n",
    "####INCLUDE FINAL, MERGED DATASET HERE####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cb0d7c-3298-497f-812f-0be18e1d556f",
   "metadata": {},
   "source": [
    "# The Ranking Model and Segmentations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db95a7ef-f31b-4050-ace7-53f1fd2a9928",
   "metadata": {},
   "source": [
    "Initially, we devised a metric using three models: Customer-Merchant Model (CN), Customer Number Model (CNM) and Customer Sampling Model (CS). The CN model would be used to predict the amount each cutomer-merchant pair would spend on a monthly basis; the CNM model in conjunction with CS would be used to predict revenue for a specific month (using a Monte-Carlo sampling method). Ultimately, we would use all of this information to predict the future revenue for any given month for a specific merchant. However, we faced a number of issues in creation of this system:\n",
    "- Customer were sparse: we found that each customer only had a few transactions per merchant, hence making it difficult to derive a predictive power\n",
    "- Memory/Technical Issues: since these databases/operations were computationally heavy, we were met with several RAM issues across group members, as well as .env and environment errors which impeded the creation process significantly \n",
    "\n",
    "As a result, we decided to change the model to the one we have now. By using several, reputable articles and studies, we were able to select and create features:\n",
    "- Prevalent in Afterpay's annual report was the notion that the majority of revenue comes from the take-rate - hence, highlighting the fact that this should be the most important aspect of our model\n",
    "- Customer Retention served as a distinguishing factor amongst the top chosen merchants - some merchants with comparitively lower revenue scores were still pinpointed by our model due to the comparitively higher retention score, which is an interesting discovery\n",
    "- Research also revealed characteristics of BNPL customers which was accounted for in our Customer Quality metric:\n",
    "    - Federal Reserve Bank of Philadelphia stated that the BNPL market is far more attractive to the younger audience\n",
    "    - Federal Reserve Bank of Philadelphia also advised us to avoid nil income customers as they are likely to default\n",
    "    - Seek low mortgage-income ratio, as mortgage stress is a strong indicator of financial position\n",
    "    - Roy Morgan (reputable market research company) stated avoid high income customers as they are unlikely to use a BNPL approach\n",
    "\n",
    "This research prompted us to weight the features accordingly. In regard to the Customer Quality metric, we found it interesting how amongst the top merchants, the mean quality of customers converges to the mean of the individual customer quality distribution. The implication is that each merchant's customers are chosen randomly from the overall customer distribution. Possibly with more realistic data, this would be more useful. \n",
    "\n",
    "(INSERT A VISUAL THAT SHOWCASES THE CONVERGING CUSTOMER QUALITY STUFF)\n",
    "\n",
    "Below is our final ranking of merchants: \n",
    "\n",
    "\n",
    "- rankings.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76727463-037b-4037-9219-975bb584cc97",
   "metadata": {},
   "source": [
    "### Segmentations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed40d536-0870-4409-8fa9-2fb6915e9d9e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b85a16c5-8fdf-473f-9f8a-b9dbdafa772e",
   "metadata": {},
   "source": [
    "### Interesting Merchants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39eb1f52-d861-4cc5-8145-c3a012fb5afc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b783f671-0dfb-4224-a155-6477f6785dfe",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
